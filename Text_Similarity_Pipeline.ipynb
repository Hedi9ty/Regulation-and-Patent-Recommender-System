{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\textmining\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('D:/python/TextMining/FUNCTION')\n",
    "from jieba_split_beta import zh_split\n",
    "from text_mining_beta import text_mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'ID' : range(4), 'Content' : ['吃葡萄不吐葡萄皮', '不吃葡萄到吐葡萄皮', \n",
    "                                                  '扁擔想綁在板凳上，板凳不讓扁擔綁在板凳上',\n",
    "                                                   '扁擔偏要綁在板凳上，板凳偏不讓扁擔綁在板凳上']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZH split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from D:\\python\\TextMining\\Dictionary\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\szwang\\AppData\\Local\\Temp\\jieba.u837dce464c01df0ad686a1dbf98c499f.cache\n",
      "Loading model cost 2.391 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add user dictionary from D:/python/TextMining/Dictionary/edu_dict.txt\n",
      "add user dictionary from D:/python/TextMining/Dictionary/company_dict.txt\n",
      "add user dictionary from D:/python/TextMining/Dictionary/geo_dict.txt\n",
      "add user dictionary from D:/python/TextMining/Dictionary/law_dict.txt\n",
      "add user dictionary from D:/python/TextMining/Dictionary/FCS_dict.txt\n",
      "add user dictionary from D:/python/TextMining/Others/Benz/car_dict.txt\n",
      "add user dictionary from D:/python/TextMining/Others/Taichung/taichung1.txt\n",
      "斷詞......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字詞數量 (未處理)：17\n"
     ]
    }
   ],
   "source": [
    "# 載入繁體版Jieba\n",
    "analyzer = zh_split('D:/python/TextMining/Dictionary/dict.txt.big')\n",
    "# 載入自定義字典\n",
    "analyzer.add_dictionary('D:/python/TextMining/Dictionary/edu_dict.txt')\n",
    "analyzer.add_dictionary('D:/python/TextMining/Dictionary/company_dict.txt')\n",
    "analyzer.add_dictionary('D:/python/TextMining/Dictionary/geo_dict.txt')\n",
    "analyzer.add_dictionary('D:/python/TextMining/Dictionary/law_dict.txt')\n",
    "analyzer.add_dictionary('D:/python/TextMining/Dictionary/FCS_dict.txt')\n",
    "analyzer.add_dictionary('D:/python/TextMining/Others/Benz/car_dict.txt')\n",
    "analyzer.add_dictionary('D:/python/TextMining/Others/Taichung/taichung1.txt')\n",
    "# 斷詞\n",
    "analyzer.split(data.Content)\n",
    "analyzer.get_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['吃', '葡萄', '不吐', '葡萄', '皮'],\n",
       " ['不吃', '葡萄', '到', '吐', '葡萄', '皮'],\n",
       " ['扁擔', '想', '綁', '在', '板凳', '上', '，', '板凳', '不讓', '扁擔', '綁', '在', '板凳', '上'],\n",
       " ['扁擔',\n",
       "  '偏要',\n",
       "  '綁',\n",
       "  '在',\n",
       "  '板凳',\n",
       "  '上',\n",
       "  '，',\n",
       "  '板凳',\n",
       "  '偏',\n",
       "  '不讓',\n",
       "  '扁擔',\n",
       "  '綁',\n",
       "  '在',\n",
       "  '板凳',\n",
       "  '上']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 斷詞結果\n",
    "analyzer.split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "analyzer.word_filter(w_len=0, # 過濾單詞字數 <= N的單詞\n",
    "                     path_word='D:/python/TextMining/Dictionary/stop_words.txt' # 停止詞字典\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['吃', '葡萄', '不吐', '葡萄', '皮'],\n",
       " ['不吃', '葡萄', '吐', '葡萄', '皮'],\n",
       " ['扁擔', '想', '綁', '板凳', '板凳', '不讓', '扁擔', '綁', '板凳'],\n",
       " ['扁擔', '偏要', '綁', '板凳', '板凳', '偏', '不讓', '扁擔', '綁', '板凳']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 停止詞過濾結果\n",
    "analyzer.split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngram......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('綁板凳', 1.0, ('綁', '板凳')),\n",
       " ('葡萄皮', 0.5, ('葡萄', '皮')),\n",
       " ('板凳板凳', 0.5, ('板凳', '板凳')),\n",
       " ('不讓扁擔', 0.5, ('不讓', '扁擔')),\n",
       " ('扁擔綁', 0.5, ('扁擔', '綁')),\n",
       " ('吃葡萄', 0.3333333333333333, ('吃', '葡萄')),\n",
       " ('葡萄不吐', 0.3333333333333333, ('葡萄', '不吐')),\n",
       " ('不吐葡萄', 0.3333333333333333, ('不吐', '葡萄')),\n",
       " ('不吃葡萄', 0.3333333333333333, ('不吃', '葡萄')),\n",
       " ('葡萄吐', 0.3333333333333333, ('葡萄', '吐')),\n",
       " ('吐葡萄', 0.3333333333333333, ('吐', '葡萄')),\n",
       " ('扁擔想', 0.3333333333333333, ('扁擔', '想')),\n",
       " ('想綁', 0.3333333333333333, ('想', '綁')),\n",
       " ('扁擔偏要', 0.3333333333333333, ('扁擔', '偏要')),\n",
       " ('偏要綁', 0.3333333333333333, ('偏要', '綁')),\n",
       " ('板凳偏', 0.3333333333333333, ('板凳', '偏')),\n",
       " ('偏不讓', 0.3333333333333333, ('偏', '不讓')),\n",
       " ('板凳不讓', 0.25, ('板凳', '不讓'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#找出潛在關鍵字\n",
    "analyzer.find_keyword(n=2)\n",
    "#潛在關鍵字結果\n",
    "analyzer.add_word #會按機率跳出潛在的組合字詞，可以透過人工方式加入字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 text monong 分析器\n",
    "tm = text_mining(analyzer.split_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字詞數量 (未處理)：13\n"
     ]
    }
   ],
   "source": [
    "# get dictionary\n",
    "tm.get_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_VECTOR :  [[(0, 1), (1, 1), (2, 1), (3, 2)], [(2, 1), (3, 2), (4, 1), (5, 1)], [(6, 1), (7, 1), (8, 2), (9, 3), (10, 2)], [(6, 1), (8, 2), (9, 3), (10, 2), (11, 1), (12, 1)]]\n",
      "TFIDF_VECTOR :  [[(0, 0.5547001962252291), (1, 0.5547001962252291), (2, 0.2773500981126146), (3, 0.5547001962252291)], [(2, 0.2773500981126146), (3, 0.5547001962252291), (4, 0.5547001962252291), (5, 0.5547001962252291)], [(6, 0.21320071635561041), (7, 0.42640143271122083), (8, 0.42640143271122083), (9, 0.6396021490668313), (10, 0.42640143271122083)], [(6, 0.19611613513818404), (8, 0.3922322702763681), (9, 0.5883484054145521), (10, 0.3922322702763681), (11, 0.3922322702763681), (12, 0.3922322702763681)]]\n"
     ]
    }
   ],
   "source": [
    "# 計算TF\n",
    "tm.CounterVector()\n",
    "print('TF_VECTOR : ', tm.TF_Vector)\n",
    "# 計算TFIDF\n",
    "tm.TfidfVector()\n",
    "print('TFIDF_VECTOR : ', tm.TFIDF_Vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\textmining\\lib\\site-packages\\gensim\\models\\doc2vec.py:362: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n"
     ]
    }
   ],
   "source": [
    "# LSI\n",
    "lsi_result, lsi_model = tm.LSI(tm.TFIDF_Vector, \n",
    "                               n_dimension = 10 #維度\n",
    "                              )\n",
    "# Doc2vec\n",
    "doc_result, doc_model = tm.doc2vec(vector_size = 10, #維度\n",
    "                                   epochs = 50, dbow_words= 1, dm=0, iter=1, window=5\n",
    "                                  )\n",
    "doc_result = [list(enumerate(row)) for row in doc_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.8320502943378434), (2, 0.5547001962252297)]\n",
      "[(1, 0.832050294337844), (2, -0.5547001962252287)]\n",
      "[(0, 0.936113724129281), (3, 0.3516974488062277)]\n",
      "[(0, 0.9361137241292808), (3, -0.3516974488062279)]\n"
     ]
    }
   ],
   "source": [
    "for i in lsi_result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.01819706), (1, -0.020900346), (2, -0.022570532), (3, -0.036192883), (4, 0.037397917), (5, 0.04011931), (6, -0.024867928), (7, 0.021837499), (8, 0.031056508), (9, -0.0046217255)]\n",
      "[(0, -0.008751348), (1, -0.040150538), (2, -0.011897086), (3, -0.013543828), (4, 0.026310751), (5, 0.017855063), (6, -0.028779281), (7, 0.027169295), (8, -0.0032822862), (9, -0.032448743)]\n",
      "[(0, -0.014858185), (1, -0.035465114), (2, 0.04442787), (3, -0.033023227), (4, -0.004807039), (5, -0.02609597), (6, -0.007482801), (7, -0.024609761), (8, -0.048068736), (9, -0.020046808)]\n",
      "[(0, 0.01056307), (1, 0.0130757475), (2, 0.017204788), (3, -0.017703114), (4, 0.0019285983), (5, -0.030628184), (6, -0.013498587), (7, 0.023402838), (8, 0.037826106), (9, -0.02784463)]\n"
     ]
    }
   ],
   "source": [
    "for i in doc_result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000001 , 0.38461545, 0.        , 0.        ],\n",
       "       [0.38461545, 1.0000001 , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.99999994, 0.7526178 ],\n",
       "       [0.        , 0.        , 0.7526178 , 0.99999994]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lsi similarity\n",
    "tm.sim(lsi_result, lsi_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.6823818 , -0.28456056,  0.19250275],\n",
       "       [ 0.6823818 ,  1.        ,  0.17691316,  0.13963681],\n",
       "       [-0.28456056,  0.17691316,  0.99999994, -0.03379012],\n",
       "       [ 0.19250275,  0.13963681, -0.03379012,  0.99999994]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc2vec similarity\n",
    "tm.sim(doc_result, doc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.53349864, -0.14228028,  0.09625138],\n",
       "       [ 0.53349864,  1.        ,  0.08845658,  0.06981841],\n",
       "       [-0.14228028,  0.08845658,  0.99999994,  0.35941383],\n",
       "       [ 0.09625138,  0.06981841,  0.35941383,  0.99999994]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensemble\n",
    "(tm.sim(lsi_result, lsi_result) + tm.sim(doc_result, doc_result)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "textmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
